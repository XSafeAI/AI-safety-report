# L-Safe

**L-Safe** is a unified framework for evaluating the safety of large language models (LLMs) and multimodal models across multiple dimensions, including adversarial robustness, benchmark alignment, regulatory compliance, and multilingual safety.


## Repository Structure
```text
l-safe/
├── adversarial/
│   └── README.md        # Adversarial (jailbreak) evaluation methods
├── benchmark/
│   └── README.md        # Standard safety benchmark evaluations
├── compliance/
│   └── README.md        # Regulatory and policy compliance checks
├── multilingual/
│   └── README.md        # Multilingual safety evaluation
└── README.md            # Project overview (this file)
